% Review: Chris, Aditi, Russell, Yanni, Enbody, Brian Haas, Jared,
% andy cameron, rebecca, tringe, emsch, emrich, dworkin.

% send final to: gasser, pws, gpgc, woyke, cheng, dodgson.

% @@spellcheck!
% VERIFY coverage figure; 0 coverage
% todo:
%   3-pass trinity/oases
%   mouse k-mer foo
% @schizo mapping coverage graph for mrnaseq?
% @@graph of k-mer count vs data
%
% @ mouse missing k-mers
% @ do micro dn in 1gb.
% @ why more transcripts??

% Template for PLoS
% Version 1.0 January 2009
%
% To compile to pdf, run:
% latex plos.template
% bibtex plos.template
% latex plos.template
% latex plos.template
% dvipdf plos.template

\documentclass[10pt]{article}

% amsmath package, useful for mathematical formulas
\usepackage{amsmath}
% amssymb package, useful for mathematical symbols
\usepackage{amssymb}

% graphicx package, useful for including eps and pdf graphics
% include graphics with the command \includegraphics
\usepackage{graphicx}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

\usepackage{color} 

% Use doublespacing - comment out for single spacing
%\usepackage{setspace} 
%\doublespacing


% Text layout
\topmargin 0.0cm
\oddsidemargin 0.5cm
\evensidemargin 0.5cm
\textwidth 16cm 
\textheight 21cm

% Bold the 'Figure #' in the caption and separate it with a period
% Captions will be left justified
\usepackage[labelfont=bf,labelsep=period,justification=raggedright]{caption}

% Use the PLoS provided bibtex style
\bibliographystyle{plos2009}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother


% Leave date blank
\date{}

\pagestyle{myheadings}
%% ** EDIT HERE **


%% ** EDIT HERE **
%% PLEASE INCLUDE ALL MACROS BELOW

%% END MACROS SECTION

\begin{document}

% Title must be 150 characters or less
\begin{flushleft}
{\Large
\textbf{Digital Normalization of Short Shotgun Sequences Facilitates
{\em de novo} Sequence Assembly}
}
% Insert Author names, affiliations and corresponding author email.
\\
C. Titus Brown$^{1,\ast}$, 
Adina Howe$^{2}$,
Qingpeng Zhang$^{3}$,
Alexis B. Pyrkosz$^{4}$,
Timothy H. Brom$^{3}$
\\
\bf{1} Departments of Computer Science and Engineering/Microbiology and Molecular Genetics, Michigan State University, East Lansing, MI, USA
\\
\bf{2} Departments of Microbiology and Molecular Genetics/Crop and Soil Sciences, Michigan State University, East Lansing, MI, USA
\\
\bf{3} Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA
\\
{\bf{4} USDA Avian Disease and Oncology Laboratory, East Lansing, MI, USA}
\\
$\ast$ E-mail: Corresponding ctb@msu.edu
\end{flushleft}

% Please keep the abstract between 250 and 300 words
\section*{Abstract}

{\bf Background:} Deep shotgun sequencing and analysis of genomes,
transcriptomes, amplified single-cell genomes, and metagenomes enable
the sensitive investigation of a wide range of biological
phenomena. However, it is increasingly difficult to deal with the volume of data
emerging from deep short-read sequencers, in part because of random
and systematic sampling variation as well as many sequencing errors.
These challenges have led to the development of entire new classes of
short-read mapping tools such as Bowtie and BWA, as well as new de
novo assemblers such as ABySS, Velvet, SOAPdenovo, ALL-PATHS, and SGA.
Even newer assembly strategies for dealing with transcriptomes,
single-cell genomes, and metagenomes have also emerged.  Despite these
advances, algorithms and compute capacity continue to be challenged by
the continuing improvements in sequencing technology.
\\
\\
{\bf Methodology and Principal Findings:} We describe an approach we term
digital normalization, a single-pass computational algorithm that
reduces both sampling variation and the number of errors in deep sequencing data. Digital normalization substantially
reduces the size of data sets and accordingly decreases the memory and time
requirements for {\em de novo} sequence assembly, all without significantly
impacting content of the generated contigs.  Moreover, several data sets
yield a significant improvement in assembly contiguity after digital normalization
is performed.
\\
\\
{\bf Conclusions and Significance:} Digital normalization is an
effective theoretical and practical filtering technique for decreasing computing
requirements for {\em de novo} sequence assembly.  We demonstrate this
on the assembly of microbial genomes, amplified single-cell genomic data,
and transcriptomic data.  The software is freely available for use and
modification.

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLoS ONE authors please skip this step. 
% Author Summary not valid for PLoS ONE submissions.   
\section*{Author Summary}

\section*{Introduction}

The ongoing revolution in DNA sequencing technologies has led to a new
problem: how do we analyze the resulting very large sequence data sets
quickly and efficiently? These data sets contain millions to billions
of short reads, and have high error rates and substantial sampling
biases.  The vast quantities of deep sequencing data produced by
these new sequencing technologies are driving
computational biology to extend and adapt previous approaches to sequence
analysis.  In
particular, the widespread use of deep shotgun sequencing on
previously unsequenced genomes, transcriptomes, and metagenomes, has
resulted in the development of several new approaches to {\em de novo}
sequence assembly.

There are two basic challenges to dealing with short-read sequences
from shotgun sequencing. The first is that deep sequencing is needed
for thorough sampling. This is because shotgun sequencing samples
randomly from populations of molecules; this sampling is
generally biased by sample content,
sample preparation and sequencing technology, which requires even deeper
sequencing to overcome.  For this reason, a
human genome may require 100x coverage or more for near-complete
sampling, leading to 300 gigabase+ data sets.  Since the minimum sampling depth
is determined by the prevalence of the lowest abundance molecule, transcriptomes and
metagenomes may also require similarly deep sequencing for complete
sampling of rare molecules. This leads to extremely large amounts of
sequencing data.

The second problem with short-read shotgun sequencing is the high
error rate.  Illumina GAII has up to a 1\% error rate in its
sequences, yielding an average of one base error in every 100 bp of
data.  These errors grow linearly with the amount of data generated and
dominate novelty in high-coverage sequencing data sets.  Tracking this
novelty and resolving errors is computationally expensive.

These two problems have combined to result in a significant
computational challenge: it is now straightforward to generate data
sets that cannot easily be analyzed computationally.  While hardware approaches
to scaling existing algorithms are emerging, sequencing capacity
is continues to grow faster than computational capacity.
This arms race cannot be won only through improvements
in hardware.  What algorithmic solutions have been applied?

Analysis of the large amounts of data resulting from deep sequencing
has been tackled in a number of ways.  A new class of alignment tools,
mostly relying on the Burroughs-Wheeler transform, has been created
specifically to do ultra-fast short-read alignment to reference
sequence.  In cases where the reference sequence must be assembled de
novo from the sequence data, a number of new assemblers have been
created, including ABySS, Velvet, SOAPdenovo, ALLPATHS, and SGA.
These assemblers rely on theoretical advances such as the application
of de Bruijn graphs and compressed @LM data structures to store large
amounts of data in a computationally convenient manner.  As short-read
sequencing has been applied to single cell genomes, transcriptomes,
and metagenomes, yet another generation of assemblers has emerged to
handle reads from very abundance-biased populations of molecules;
these tools, including Trinity, Oases, MetaVelvet, Meta-IDBA, and
Velvet-SV, adopt local models of sequence coverage to help build
assemblies.  Because these tools all rely on k-mer approaches and
require exact matches between sequences, their performance is
extremely sensitive to the number of errors present in the underlying
data.  This sensitivity to errors has led to a number of error removal
and correction approaches.

Several approaches have been applied to detecting and removing or
correcting errors in large data sets \cite{pubmed21114842}.
Typically, these approaches involve finding low-abundance fixed-length
words, or k-mers, and treating them as likely errors; these errors can
then be eliminated or ``corrected'' by changing them to similar k-mers
that are in high abundance.  One challenge for these techniques is
that they require at least two complete iterations across the data
set: one iteration to count all k-mers, and a second to perform the
trimming or correction.  For extremely large data sets, this is a
significant problem: the ``noise'' from sequencing errors drives a
supra-linear increase in the number of k-mers, so as data set size
increases, the computation and memory needed to track k-mers increases
dramatically.

Here, we describe a single-pass algorithm for detection and
elimination of redundant reads in data sets that provides a method for
reducing data set size and error content, and correspondingly reduces
computational requirements for {\em de novo} assembly.  This algorithm,
which we call digital normalization, is inspired by experimental
normalization techniques developed for cDNA library preparation.  In
experimental normalization, hybridization kinetics are exploited to
reduce the copy number of highly abundant transcripts, ``normalizing''
or evening out the abundance distribution so that shotgun cloning and
sequencing recover low-abundance molecules as well as high-abundance
molecules \cite{pubmed8889548,pubmed7937745}.  In turn, digital
normalization was initially designed to deal with reads from very
abundance-biased samples such as single-cell amplified genomic DNA,
transcriptomes, and metagenomes.  Digital normalization works by
systematically removing redundant reads from data sets.  By
eliminating redundant reads, errors contained in these reads do not
accumulate in the normalized data set, and we see a correspondingly
substantial reduction in both data size and total number of errors.
We present a constant-memory implementation of this approach that
operates in time linear with the size of the input data.  We
demonstrate its effectiveness for reducing compute requirements for de
novo assembly on several real data sets, including data from a
microbial genome, MDA-amplified single-cell genomic DNA, and a mouse
transcriptome.

% Results and Discussion can be combined.
\section*{Results}

\subsection*{Median k-mer abundance of a read is a good estimator of coverage}

Suppose that we have a file of short reads that are randomly sampled
without error from a non-repetitive source genome, and we want to know
the number of times a specific genomic location is represented in the
reads.  This number, known as the coverage, is typically estimated by
assembling a reference sequence, mapping the short reads back to this
reference, and counting the number of reads that map to the location.
However, this requires assembling a reference sequence, which can be
computationally expensive.  Can we estimate the coverage of a source
genomic region solely from the reads, without having access to a
reference?

One approach to estimating coverage is to select a read overlapping
the location of interest, and then apply the following
procedure. First, choose a number $k$ such that k-mers -- words of
length $k$ -- are unique in the single-copy source genome; a k-mer
size of 20 usually serves (cite k-mer counts paper).  Next, for each
k-mer in the selected read, calculate the number of times that the
k-mer appears in {\em any} read across the entire data set.  The
abundance of these k-mers will represent the true depth of sampling of
the source genome for each k-mer.  We can then estimate the sampling
rate or coverage of the region to which the read belongs by
calculating a summary statistic such as the median of the
read's k-mer abundance distribution: see Figure \ref{fig:rankabund},
``no errors'' line. This approach relies on the observation that
because reads come from a single location in the genome, the
abundances of non-repetitive k-mers within the read are correlated.
The median provides an estimator that is robust to extreme abundance
values stemming from repeats.

We can extend this estimation technique to reads sampled {\em with}
error by observing that each single-base miscall or indel changes no
more than $k$ k-mers in the read (see Figure \ref{fig:rankabund}).  If
the errors are random and $k$ is large, these k-mers will be low
abundance in the read data set.  So, if we order the k-mers in a read
by their abundance across the entire data set, only the abundance of
the first $k$ ranked k-mers will be affected by a single error, and at
most the first $2k$ ranked k-mers will be affected by a pair of
errors.  The median of the k-mer rank-abundance distribution will have
rank $(l - k + 1) / 2$; this is greater than $k$ for $l > 3k-1$, and
greater than $2k$ for $l > 5k-1$.  Thus for read lengths of 76 bases
or longer, and a k-mer size of 20, the median k-mer abundance will
ignore low abundances caused by a single substitution error; for a read
length of 100, and a k-mer size of 20, the median k-mer abundance will
be robust to two substitution errors.

We can therefore use the median k-mer abundance of a read, measured
across the entire read data set, as an estimator of sampling depth for
the region represented by the read. How well does median k-mer
abundance correlate with the abundance calculated from read mapping?
In Figure \ref{fig:random}a, we show that for simulated 100-base reads
from an artificial (non-repetitive) genome of 400kb, sampled with a
1\% per-base error rate, the median 20-mer abundance matches both the
true coverage and the estimated coverage obtained from mapping the
reads back to the genome with bowtie ($r^2 = 0.79$).  Can we apply
this approach to reads from real genomes?

When we apply the median k-mer abundance measure to real reads sampled
from a {\em E. coli} genome, we see the same strong correlation
between median k-mer abundance and estimated coverage based on mapping
\ref{fig:random}b ($r^2=0.80$).  Points below the diagonal represent
reads with multiple errors that reduce the median k-mer count but can
still be mapped accurately, while points above the diagonal contain
high-abundance k-mers that skew the median k-mer count relative to
the mapping coverage.

This same correlation also holds for reads sampled from a non-uniform
molecular distribution such as transcriptome or metagenome shotgun
sequencing.  Figure \ref{fig:transcripts}a shows that median k-mer
abundance and number of mapped reads correlate for a simulated
transcriptome with expression across 3 orders of magnitude ($r^2 =
0.93$).  Figure \ref{fig:transcripts}b shows the same strong
correlation for a mouse mRNAseq data set ($r^2 = 0.90$).

\subsection*{The median k-mer abundance can be used to remove redundant reads.}

We can now use the median k-mer abundance to progressively accumulate
only informative (non-redundant) reads, that is, reads that add
significantly to estimated coverage.  Significantly, we can do this
{\em without a reference sequence}: no prior reference sequence is required.

Suppose we treat the read data set as an incoming stream of data
consisting of reads which we can individually accept or reject.  For
each read, we use the median k-mer abundance to estimate the coverage
of that read based on the reads accumulated so far; if this estimated
coverage is {\em below} a specified threshold, we accept the read, and
otherwise we reject it.  Accepted reads are both saved and
incorporated into the k-mer counts, while rejected reads are simply
discarded.

Algorithmically, we execute the following procedure:

\begin{verbatim}
   for read in dataset:
      if estimated_coverage(read) < C:
         accept(read)
      else:
         discard(read)
\end{verbatim}

where only accepted reads contribute to the estimated coverage.  For
any data set, this has the effect of discarding the majority of reads
that contribute to coverage $>$ C, although overlapping reads may be
accepted later on that push the estimated coverage of earlier reads
above C.  Novel reads -- reads from undersampled regions, or those
reads containing many errors -- will always be retained.

In Figure \ref{fig:coverage}, we display the estimated coverage of a
simulated genome and an {\em E. coli} genome for both raw and
normalized data, using read multimapping.  From this,
we see that digitally normalizing both simulated and real
high-coverage data has the expected effect of normalizing the
estimated coverage of the reads to C, not only for median k-mer
abundance but for read multimapping.  For both simulated and real
data, then, digital normalization downsamples the data set to only a
subset of reads -- those reads that contribute to additional coverage
below a certain threshold.  Digital normalization thus converts {\em
  high random sampling} to {\em low systematic sampling}.

At what rate are informative sequences retained?  For the {\em E. coli} data
set, Figure \ref{fig:accumulate} shows the fraction of sequences
retained by digital normalization as a function of the total number of
reads examined when normalizing to C=20 at k=20.  There is a clear
saturation effect showing that as more reads are examined, a smaller
fraction of reads is retained; by 5m reads, approximately 50-100x
coverage of {\em E. coli}, under 30\% of new reads are kept.  This
demonstrates that only a very small amount of novelty (in the form of
either new information, or the accumulation of significant errors) is
being observed as sequencing depth increases, as we would expect.

% @CTB figure 4b / transcriptome/mouse stuff?

\subsection*{Digital normalization removes many sequencing errors}

The per-base error rate of most next-generation sequencing technologies
is rather high, approaching 1-2\%.  
These
errors dramatically affect the total number of k-mers; for example, in the
simulated genomic data of 200x, a 1\% error rate leads to more than
20-fold more 20-mers in the reads than are truly present in the genome,
or approximately 20 new k-mers for each error (Table 1, row 1).
This in turn dramatically increases the memory requirements for
tracking and correcting k-mers (cite bloom filter paper).

When we perform digital normalization on such a data set, we eliminate
the vast majority of these k-mers (Table 1, row 1).  Intuitively, this is
because we are accepting or rejecting individual reads; in going from
200x random coverage to 20x systematic coverage, we are discarding
80\% of the reads containing 62\% of the errors (Table \ref{tab:normC20}, row 1).
For reads taken from a biased abundance distribution, such as with
mRNAseq, we also discard many reads, and hence many errors (Table \ref{tab:normC20}, row 2).
Conveniently, in doing so we retain nearly all real k-mers -- in other words,
we are discarding {\em data} but not {\em information}.

During the initial data set reduction we can count the k-mers in the
accepted reads.  With this information we can then execute a second pass
across the accepted reads in which we trim reads at low-abundance
k-mers, thus removing another set of likely errors.  Following this
second pass error reduction, we can then execute a second round of
digital normalization that further eliminates redundant data; this
three-pass protocol eliminates some additional errors, at the cost of
very few real k-mers (Table \ref{tab:normC5}).

Why use this three-pass protocol rather than simply normalizing to the
lowest desired coverage in the first pass?  We find that error
trimming after single-pass normalization to a coverage of $\tilde 5$
removes many more real k-mers, because there will be many regions in
the genome that by chance have yielded 5 reads with errors in them;
these reads are then trimmed in the abundance-trimming step,
eliminating coverage of the corresponding regions.  By normalizing
first to a higher coverage of 20, removing errors, and only then
reducing coverage to 5, digital normalization can still recover reads
for most regions.  Computationally, the three-pass protocol is not
considerably more expensive than the single-pass protocol: the first
pass of digital normalization discards substantial amounts of data as
well as many erroneous k-mers, so later passes are always less time
and memory intensive than the first pass.

Interestingly, this three-pass protocol removed many more real k-mers
from the simulated mRNAseq data than from the simulated genome (351 of
48,100 true (0.7\%) lost in the mRNAseq, vs 4 of 399,981 lost
(.000001\%) in the genome).  While still only a tiny fraction of the
total number of real k-mers, the difference is striking -- the
simulated mRNAseq sample lost k-mers at almost 1000-fold the rate of
the simulated genomic sample!  Upon further investigation, all but one
of the lost k-mers were located within 20 bases of the ends of the
source sequences; see Figure \ref{fig:endloss}.  This is because
digital normalization cannot distinguish between erroneous k-mers and
k-mers that are undersampled due to edge effects.  In the case of the
simulated genome, which was generated as one large chromosome, the
effect is negligible, but the simulated transcriptome was generated as
100 transcripts of length 500.  This added 99 end sequences over the
genomic simulation, which presents much more opportunity for k-mer
loss from the ends of sequences.

While the three-pass protocol is very effective at removing erroneous k-mers,
for shallow samples it may be too stringent.  For example, the mouse
mRNAseq data set contains only 100m reads, which may not be enough to
thoroughly sample the rarest molecules; in this case the abundance trimming
may remove real k-mers as well as erroneous k-mers.  Therefore we only
apply the three-pass protocol to extremely deep genome samples below,
and use the single-pass digital normalization for the mouse transcriptome.

\subsection*{Digital normalization facilitates {\em de novo} assembly of genomic data}

We applied the three-pass digital normalization and error trimming
protocol to real data sets from Chitsaz et al (2011).  The data sets
were all high coverage, and included sequences from a single colony of
{\em E. coli}, MDA-amplified single-cell DNA from {\em S. aureus}, and
MDA-amplified DNA from a single cell of an uncultured {\em
  Deltaprotobacterium}.  Digital normalization was performed in 1gb of
memory and took about 1 min per million reads.  For all three samples,
the number of reads remaining after digital normalization was reduced
by at least 30-fold, while the memory and time requirements were
reduced 10-100x.  (Here we used a more recent versions of Velvet
rather than Velvet-SC, because optimizations have been added to Velvet
since the Velvet-SC fork.)

Despite this dramatic reduction in data set size and computational
requirements for assembly, both the {\em E. coli} and {\em S. aureus}
assemblies overlapped with the known reference sequence by more than
98\%.  This demonstrates that little or no information was lost during
the process of digital normalization; moreover, it appears that
digital normalization does not significantly affect the assemblers.
(Note that we did not perform scaffolding, since the digital
normalization algorithm does not take into account paired-end
sequences, and could mislead scaffolding approaches.  Therefore, these
results cannot directly be compared to those in Chitsaz et al.)

The {\em Deltaproteobacteria} sequence also assembled well, with
98.8\% sequence overlap with the results from Chitsaz et al.
Interestingly, only 30kb of sequence of the sequence assembled with
Velvet-SC in Chitsaz et al. was missing, while an additional 360kb of
sequence was assembled only in the normalized samples.  Of the missing
30kb from the Velvet-SC results, 10\% matched via TBLASTX to a nearby
{\em Deltaproteobacteria} assembly, while more than 40\% of the
additional 360kb from the assembly of the normalized reads matched the
nearby {\em Deltaproteobacteria}.  This suggests that for this sample,
digital normalization is competitive with Velvet-SC in terms of contig
assembly of sequence reads from MDA-amplified DNA.

@@ tables or anything?

\subsection*{Digital normalization facilitates {\em de novo} assembly of transcriptomes}

We next applied single-pass digital normalization to published yeast
and mouse mRNAseq data sets, reducing them to 20x coverage for k=20.
Digital normalization on these samples used 8gb of memory and took
about 1min per million reads.  We then assembled both the original and
normalized sequence reads with Oases and Trinity, two {\em de novo}
transcriptome assemblers (Table \ref{tab:dntrans}).  (Note that due to
differing execution parameters, the Oases runtimes cannot be compared
to the Trinity runtimes.)

In both cases the computational resources necessary to complete an
assembly were reduced (Table \ref{tab:dntrans}), but normalization had
different effects on performance of the assemblers.  For Oases, both
assembly time and memory were significantly reduced, while for Trinity
only the time was significantly reduced.

The resulting assemblies differed in summary statistics for mouse, with about
20\% difference in the number of short contigs between the normalized
and unnormalized Trinity and Oases assemblies (Table
\ref{tab:dntrans0}).
Curiously, digital normalization resulted in {\em fewer} contigs in
the normalized Oases assembly, and {\em more} contigs in the
normalized Trinity assembly, indicating that the assemblers were
affected differently by digital normalization.
Using a local-alignment-based overlap analysis (see Methods) we found
very little difference in sequence content between the pre- and post-
normalization assemblies in mouse:
the normalized Oases assembly had an 98.5\% overlap with the
unnormalized Oases assembly, while the normalized Trinity assembly had
a 97\% overlap with the unnormalized Trinity assembly.

To further investigate the differences between transcriptome
assemblies caused by digital normalization, we looked at the
sensitivity with which long transcripts were recovered
post-normalization.  When comparing the normalized assembly to the
unnormalized assembly in yeast, Trinity lost only 3\% of the sequence
content in transcripts greater than 300 bases, but 10\% of the
sequence content in transcripts greater than 1000 bases.  However,
Oases lost less than 0.7\% of sequence content at 300 bases and at
1000 bases.  In mouse, we see the same pattern.
This suggests that the change in summary statistics for
Trinity is caused by fragmentation of long transcripts into shorter
transcripts, while the difference for Oases is caused by loss of
splice variants.  Indeed, this
loss of splice variants should be expected, as mapping-based analysis
of splice variants has shown that there are many low-prevalence splice
variants present in deep sequencing data \cite{pubmed21151575}.
Interestingly, in yeast we recover {\em more} transcripts after
digital normalization; these transcripts appear to be additional splice
variants.

% @@table of splice foo?  YES.

The difference between Oases and Trinity results show that Trinity is
more sensitive to digital normalization than Oases: digital
normalization seems to cause Trinity to fragment long transcripts.
Why?  One potential issue is that Trinity only permits k=26 for
assembly, while normalization was performed at k=20; digital
normalization may be removing 26-mers that are important for Trinity's
path finding algorithm.  Alternatively, Trinity may be more sensitive
than Oases to the change in coverage caused by digital normalization.
Regardless, the strong performance of Oases on digitally normalized
samples, as well as high retention of k-mers (Table \ref{tab:normC20})
suggests that the primary sequence content for the transcriptome remains
in the normalized sample.

\section*{Discussion}

\subsection*{Digital normalization discards redundant data and removes errors}

Digital normalization is a simple, low-memory computational technique
for removing redundant data from shotgun sequencing data sets.  It
does so by using a reference-free estimator of per-read coverage, and
then discarding reads that increase coverage past a specified point,
i.e. are redundant.  This has the effect of systematically
downsampling high-coverage samples into low-coverage samples without
losing information.˜

By discarding reads as redundant, digital normalization decreases the
size of the data set that must be considered by downstream analyses
such as {\em de novo} assembly.  Moreover, by using an approximate
measure of coverage to eliminate entire reads, digital normalization
also eliminates many errors, which helps scale downstream analyses
further.

Digital normalization is similar in concept to experimental
normalization, which has been used for mRNA sequencing
\cite{pubmed8889548,pubmed7937745}. However, digital normalization is
applied computionally -- after sequencing.  In contrast to
experimental normalization, which affects the source molecule, digital
normalization still requires deep sequencing to sample rare molecules;
however, digital normalization confers several advantages over
experimental normalization, including the ability to recover source
molecule abundances from (digitally) discarded reads.

The computational cost of digital normalization is also quite low.  In
particular, the memory required for digital normalization is always
less than the memory required for assembly.  This is for two reasons:
first, digital normalization simply counts fewer k-mers; and second,
we use a memory efficient k-mer counting scheme based on a CountMin
Sketch.  While the time required for digital normalization is in some
cases approximately the same as that required for a single round of
assembly, we note that often one must try assembly with many different
parameters in order to determine the best choice; digital normalization
need only be performed once, prior to any assembly.

\subsection*{Digital normalization dramatically scales {\em de novo} assembly}

The results from applying digital normalization to read data sets
prior to {\em de novo} assembly are extremely good: digital normalization
reduces the computational requirements (time and memory) for assembly
considerably, without substantially affecting the assembly results.
Why is this?

First, by discarding reads that do not contribute significant
additional information, digital normalization removes the majority of
reads without significantly affecting the real information content of
the read data set.  This reduces the time required to load the data.
Moreover, digital normalization provides a simple principled way to
combine data from multiple samples to maximize the sensitivity of
assembly.

Second, by discarding the majority of reads from deep sequencing data
sets, digital normalization eliminates sequencing errors contained in
those reads.  These sequencing errors add significantly to memory
usage for assemblers like Velvet, so eliminating them reduces the
memory required for assembly.

Third, by converting random sampling to systematic sampling and
normalizing the coverage, digital normalization corrects bias in read
coverage, including that due to sampling bias and underlying
non-uniform molecule distributions.  This allows single-genome
assemblers such as Velvet to be applied to sequences from samples with
biased source distributions such as mRNAseq and MDA-amplified genomes,
as well as metagenomic samples from mixed-abundance populations,
without modification.

Fourth, digital normalization is directly congruent to the de Bruijn
graph approach used in many modern assemblers.  Because de Bruijn
graph assemblers use k-mer connectivity to build contigs, and k-mer
novelty is used by digital normalization as the criterion for
retaining a read, the graph structure should not be changed
significantly by normalization.  Thus we believe that digital
normalization will be a generally effective approach for preprocessing
data for {\em any} de Bruijn graph assembler, although in this
study we only demonstrate this for a few specific assemblers.

% @@show figure of converted coverage from nonuniform distribution! SICB

The net effect of digital normalization is to significantly reduce
computational requirements for {\em de novo} assembly.  This is not a
trivial consideration: as Salzberg et al. (2012) state in the GAGE
paper, ``For larger genomes, the choice of assemblers is often limited
to those that will run without crashing'' \cite{pubmed22147368}.  As
data quantity from next-generation sequencing increases, assembly
becomes ever more challenging; the ability to build transcriptomes and
single- cell genomes quickly and effectively in the face of large
quantities of data is important.  Because digital normalization
converts the assembly problem from a computational problem that scales
with the volume of data to a computational problem that scales with
the size of the underlying genome or transcriptome, it provides a
potential long-term solution to the problem of {\em de novo} assembly.

% @figure out how to make strong argument here for assembly.

\subsection*{Digital normalization drops terminal k-mers and removes isoforms}

Digital normalization is not perfect -- some real information is lost,
including terminal k-mers and low-abundance isoforms.  Moreover, we
predict a number of other failure modes: because k-mer approaches
demand strict sequence identity, we would expect datasets from highly
polymorphic organisms or populations to be challenging.  In addition,
digital normalization will discriminate against highly repetitive
sequences.  However, it is worth noting that many of these are
challenges for most assemblers: recovering low-abundance isoforms from
mRNAseq, assembling genomes from highly polymorphic organisms, and
dealing well with repeats are all active areas of research @cite dbg
repeat stuff.  Below, we discuss some potential ways to improve digital
normalization.

% @@retention of errors at a certain point.

\subsection*{Conclusions}

Digital normalization is an effective demonstration of the basic
observation that, in theory, at most 2x coverage is required to
assemble a genome -- it just has to be {\em systematic} coverage.
Here we have chosen to normalize samples to 5-20x, primarily to ensure
$>$ 2x systematic coverage in the face of sequencing errors.
Normalization and assembly of data is significantly more
computationally efficient than assembly of the raw reads.
Nonetheless, assembly of normalized data results in almost identical
assemblies for genomic data, and very similar assemblies for
transcriptomic data.  Note that discarded data can also be retained
for later quantification via mapping.

We have implemented digital normalization as a {\em prefilter}, such
that multiple assemblers may be used to assemble the normalized data.
While here we have only benchmarked a limited set of assemblers --
Velvet, Oases, and Trinity -- in theory digital normalization should
apply to any assembler, and should yield substantial memory gains for
any de Bruijn graph assembler.  Even if digital normalization does not
do so, any general approach such as this for reducing data set size
prior to analysis will be important as deep sequencing technologies
increase in throughput.

The method of digital normalization described above relies on a k-mer
based, reference-independent way of measuring the coverage of
individual reads.  Any reference-free progressive approach to
estimating coverage could serve; for example, an alignment-based
approach, in which reads were progressively used to build alignments
with error tolerance, may provide an alternative implementation that
is less sensitive to errors and does not eliminate splice variants or
k-mers at the end of sequences.  Our current approach also ignores
quality scores; a ``q-mer'' counting approach as in Quake, in which
k-mer counts are weighted by quality scores, could also be easily
adapted \cite{pubmed21114842}.

Here we have primarily tackled the problem of {\em contig assembly},
because that is the most computationally challenging part of {\em de
  novo} assembly.  Scaffolding, in which contigs are joined together
based on paired-end information, is necessary for generating longer
sequences in repeat-rich genomes.  Here, we have only examined the
results of digital normalization on microbial genomes, which can yield
good assemblies even without scaffolding.  However, in principle we
see no reason that digital normalization cannot be applied to contig
generation for eukaryotic genome assembly in the future.

\subsection*{Digital normalization is widely applicable and computationally convenient}

Digital normalization uses a reference-independent estimator of
coverage, so it can be applied {\em de novo} to {\em any} data set.  The
approach is extremely computationally convenient: the runtime
complexity is linear with respect to the data size, and perhaps more
importantly it is {\em single-pass}: the algorithm does not need to
look at any read more than once.  Moreover, because reads are
accumulated sub-linearly, errors do not accumulate quickly and overall
memory requirements for digital normalization should grow slowly with
data set size.  Note also that while the algorithm presented here is
not perfectly parallelizable, efficient distributed k-mer counting is
straightforward and it should be possible to scale digital normalization
across multiple machines \cite{pubmed19357099}.

The first pass of digital normalization is implemented as an online
streaming algorithm in which reads are examined once.  Streaming
algorithms are very useful for solving data analysis problems in which
the data is too large to easily be transmitted, computed upon, or
stored.  Here, we implement the streaming algorithm using a constant
memory data ``sketch'' data structure, CountMin Sketch.  By combining
a single-pass algorithm with a constant-memory data structure, we
provide solution to sequence data analysis with both (linear) time and
(constant) memory guarantees. Moreover, because the false positive
rate of the CountMin Sketch data structure is well understood and easy
to predict, we can provide {\em data quality guarantees} as well.
These kinds of guarantees are immensely valuable from an algorithmic
perspective, because they provide a robust foundation for further work
(c.f. Halting Problem).

We note that the general concept of removing redundant {\em data}
while retaining {\em information} is a form of ``lossy compression''
used widely in image processing and video compression (@@ compressed
sensing).  This approach has very broad applicability.  For example,
digital normalization can be done prior to homology search on
unassembled reads, potentially reducing the computational requirements
for e.g. BLAST and HMMER without loss of sensitivity (MG-RAST).
Digital normalization may also be applicable for merging multiple
different read data sets from different read technologies, in order to
discard entirely redundant sequences and retain only sequences
containing ``new'' information.  These approaches remain to be
explored in future work.

\section*{Misc foo}

\begin{verbatim}

do missassembly analysis/a5? look at a5 for refs...
NOT just error removal: smooths things out.

ht size/memory req scales with original genome size?
show graph of accumulation of reads vs new k-mers - linear??

longer sequences mention?

alexis:

Possible fail data set: a group of isoforms that collectively have no
unique kmers and are expressed at drastically different levels.  Might
not even need random sampling and errors.

Exon1Exon2Exon3 (10)
Exon2Exon3                (100)
Exon1Exon2             (1000)

In the case where two regions are represented as AXXXB and CXXXD: the
median might start getting screwy when XXX will generate more kmers
than AXX + XXB.  Say AXX generates 50 kmers, then XXX is long enough
to generate 101 kmers, then XXB generates 50 kmers.  The AXX and XXB
kmers are unique to the dataset.  XXX is shared.  All unique kmers
will be eliminated along with all but one shared kmer.

---

Aditi comments: emphasize lightweightness of diginorm.

---

Talk about specificity/sensitivity/FP/TP.

talk about quake + transcriptomes

\end{verbatim}

% You may title this section "Methods" or "Models". 
% "Models" is not a valid title for PLoS ONE authors. However, PLoS ONE
% authors may use "Analysis" 
\section*{Materials and Methods}

All online materials below are available electronically via http://@@@/.

\subsection*{Data sets}

The {\em E. coli}, {\em S. aureus}, and {\em Deltaproteobacteria} data sets
were published by Chitsaz et al. \cite{pubmed21926975}, and downloaded
from http://bix.ucsd.edu/projects/singlecell/.  The mouse data set was
published by Grabherr et al. \cite{pubmed21572440} and downloaded
from http://trinityrnaseq.sf.net/.  All data sets were used without
modification for the analyses in this paper.

The simulated genome and transcriptome were generated from a uniform
AT/CG distribution.  The genome consisted of a single chromosome
400,000 bases in length, while the transcriptome consisted of 100
transcripts of length 500.  100-base reads were generated uniformly
from the genome to an estimated coverage of 200x, with a random 1\%
per-base error.  For the transcriptome, 1 million reads of length 100
were generated from the transcriptome at relative expression levels of
10, 100, and 1000, with transcripts assigned randomly with equal
probability to each expression group; these reads also had a 1\%
per-base error.

\subsection*{Scripts and software}

All simulated data sets and all analysis summaries were generated by
Python scripts, which are available at http://github.com/ctb/@@@/.
Digital normalization and k-mer analyses were performed with the khmer
software package, written in C++ and Python, available at
http://github.com/ctb/khmer/, branch @@@.  khmer is Copyright (c) 2010
Michigan State University, and is free software available for
distribution, modification, and redistribution under the BSD license.

Mapping was performed with bowtie v0.12.7 \cite{pubmed19261174}.
Genome assembly was done with velvet 1.2.01 \cite{pubmed18349386}.
Transcriptome assembly was done with velvet 1.1.05/oases 0.1.22 and
Trinity, head of branch on 2011.10.29.

Graphs and correlation coefficients were generated using matplotlib
v1.1.0, numpy v1.7, and ipython notebook v0.12 @@cite; the ipython
notebook file and all data summaries necessary to regenerate the
figures are available at http://github.com/ctb/@@@/.

\subsection*{Results}

The complete assemblies from the {\em E. coli}, {\em S. aureus}, the uncultured
{\em Deltaproteobacteria}, mouse, and yeast data sets are available from
http://@@@/.

\subsection*{k-mer analysis parameters}

The khmer software uses a CountMin Sketch data structure to count k-mers,
which requires a fixed memory allocation (@@Cormode et al.).  In all cases
the memory usage was fixed such that the calculated false positive rate was
below 0.01.  By default k was set to 20.

\subsection*{Read coverage estimates and comparisons}

Genome and transcriptome coverage was calculated by mapping all reads
to the reference with bowtie ({\tt -a --best --strata}) and then
computing the per-base coverage in the reference.  Read coverage was
computed by then averaging the per-base reference coverage for each
position in the mapped read; where reads were mapped to multiple
locations, a reference location was chosen randomly for computing
coverage.  Median k-mer counts were computed with khmer as described
in the text.  Artificially high counts resulting from long stretches
of Ns were removed after the analysis.

Correlations between median k-mer counts and mapping coverage were
computed using numpy.corrcoef; see calc-r2.py script.

\subsection*{Normalization and assembly parameters}

In Table \ref{tab:dngenome}, the assembly k parameter for Velvet was:
k=45 for {\em E. coli}; k=41 for {\em S. aureus} single cell; and k=39
for {\em Deltaproteobacteria} single cell.  @@dn k, dn memory.
{\em E. coli} kak was 31.  (@@@ TABLE) @@@

In Table \ref{tab:dntrans}, the assembly k parameter for Oases (both
pre/post digital normalization) was k=23.  Trinity was used at the
default k=26.

\subsection*{Assembly overlap}

Assembly overlap was computed by first using NCBI BLASTN to build local
alignments for two assemblies, then filtering for matches with bit scores
$>$ 200, and finally computing the fraction of bases in each assembly
with at least one alignment.  Total fractions were normalized to
self-by-self BLASTN overlap identity to account for BLAST-specific
sequence filtering.

\subsection*{Compute requirement estimation}

Execution time was measured using real time from Linux bash 'time'.
Peak memory usage was estimated either by the 'memusg' script from
stackoverflow.com, peak-memory-usage-of-a-linux-unix-process, included
in the khmer repository; or by the Torque queuing system monitor, for
jobs run on MSU's HPC system.  While several different machines were
used for analyses, comparisons between unnormalized and normalized
data sets were always done on the same machine.

% Do NOT remove this, even if you are not including acknowledgments
\section*{Acknowledgments}

%@@ Chris Hart, James M. Tiedje, USDA * 2, NSF, BEACON, Brian Haas.
% MDA folk
% postdoc fellow.

%This work was funded in part by the DOE Great Lakes Bioenergy Research Center
%(DOE Office of Science BER DE-FC02-07ER64494), Michigan State University
%AgBioResearch and..I'm checking on the NIH one.

% NSF Postdoctoral Fellowship Award # 0905961


%\section*{References}
% The bibtex filename
\bibliography{diginorm}

\newpage

\section*{Figure Legends}

\begin{figure}
\centerline{\includegraphics[width=4in]{diginorm-ranks.pdf}}
\caption{
{\bf Representative rank-abundance distributions for k-mers from a read with no errors,
a read with a single substitution error, and a read with multiple
substitution errors.}}
\label{fig:rankabund}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\centerline{\includegraphics[width=4in]{diginorm-sim-genome.pdf}}
\centerline{\includegraphics[width=4in]{diginorm-ecoli-genome.pdf}}
\end{center}
\caption{
{\bf Mapping and k-mer coverage measures correlate for simulated genome
data and a real {\em E. coli} data set (5m reads).  Simulated data $r^2 = 0.79$; {\em
E. coli} $r^2 = 0.80$.}
}
\label{fig:random}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\centerline{\includegraphics[width=4in]{diginorm-sim-transcr.pdf}}
\centerline{\includegraphics[width=4in]{diginorm-mouse-transcr.pdf}}
\end{center}
\caption{
{\bf Mapping and k-mer coverage measures correlate for simulated transcriptome data as well as real mouse transcriptome data. Simulated data $r^2 = 0.93$;
mouse transcriptome $r^2 = 0.90$.}
}
\label{fig:transcripts}
\end{figure}

\begin{figure}
\centerline{\includegraphics[width=4in]{diginorm-coverage.pdf}}
\caption{
{\bf Coverage distribution of simulated and real genomes, calculated from mapped reads before and after normalization (k=20, C=20).}}
\label{fig:coverage}
\end{figure}

\begin{figure}
\centerline{\includegraphics[width=4in]{diginorm-accumulation.pdf}}
\caption{
{\bf Fraction of reads kept when normalizing the {\em E. coli} dataset to C=20 at k=20.}}
\label{fig:accumulate}
\end{figure}

\begin{figure}
\centerline{\includegraphics[width=4in]{diginorm-endbias.pdf}}
\caption{
{\bf K-mers at the ends of sequences are lost during digital normalization.}}
\label{fig:endloss}
\end{figure}

\section*{Tables}

\begin{table}[!ht]
\caption{
\bf{Digital normalization to C=20 removes many erroneous k-mers from sequencing data sets.  Numbers
in parentheses indicate number of true k-mers lost at each step, based on known reference (if available).}}
\begin{tabular}{|l|c|c|c|c|}
Data set & True 20-mers & 20-mers in reads & 20-mers at C=20 & \% reads kept\\
\hline \\
Simulated genome & 399,981 & 8,162,813 & 3,052,007 (-2) & 19\% \\
Simulated mRNAseq & 48,100 & 2,466,638 (-88) & 1,087,916 (-9) & 4.1\% \\
{\em E. coli} genome & 4,542,150 & 175,627,381 (-152) & 90,844,428 (-5) & 11\% \\
Yeast mRNAseq & 10,631,882 & 224,847,659 (-XX) & 179,797,797 (-XX) & 9.3\% \\
Mouse mRNAseq & 43,830,642 & 709,662,624 (-XX) & 612,275,102 (-XX) & 26.4\% \\
\end{tabular}
\begin{flushleft}
\end{flushleft}
\label{tab:normC20}
\end{table}

%@ be sure to discuss poor removal of k-mers from yeast/mouse!

%%%%%%%%%%%%%%%%%

\begin{table}[!ht]
\caption{
\bf{Three-pass digital normalization removes most erroneous k-mers}}
\begin{tabular}{|l|c|c|c|c|}
Data set & True 20-mers & 20-mers in reads & 20-mers remaining & \% reads kept\\
\hline \\
Simulated genome & 399,981 & 8,162,813 & 453,588 (-4) & 5\% \\
Simulated mRNAseq & 48,100 & 2,466,638 (-88) & 182,855 (-351) & 1.2\% \\
{\em E. coli} genome & 4,542,150 & 175,627,381 (-152) & 7,638,175 (-23) & 2.1\% \\
Yeast mRNAseq & 10,631,882 & 224,847,659 (-XX) & 10,532,451 (-XX) & 2.1\% \\
Mouse mRNAseq & 43,830,642 & 709,662,624 (-XX) & 42,350,127 (-XX) & 7.1\% \\
\end{tabular}
\begin{flushleft}
\end{flushleft}
\label{tab:normC5}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[!ht]
\caption{
\bf{Three-pass digital normalization reduces computational requirements for contig assembly of genomic data.}}
\begin{tabular}{|l|c|c|c|c|}

Data set & N reads pre/post & Assembly time pre/post & Assembly memory pre/post \\
\hline \\
%{\em E. coli} subset & 5m / 0.4m & 235s / 24s (9.8x) & 2.7gb / 0.4gb (6.8x) \\
{\em E. coli} & 31m / 0.6m & 1040s / 63s (16.5x) & 11.2gb / 0.5 gb (22.4x) \\ 
{\em S. aureus} single-cell & 58m / 0.3m & 5352s / 35s (153x) & 54.4gb / 0.4gb (136x) \\
{\em Deltaproteobacteria} single-cell & 67m / 0.4m & 4749s / 26s (182.7x) & 52.7gb / 0.4gb (131.8x) \\

\end{tabular}
\begin{flushleft}
\end{flushleft}
\label{tab:dngenome}
\end{table}



%%%%%%%%%%%%%%%%%%

\begin{table}[!ht]
\caption{
\bf{Single-pass digital normalization to C=20 reduces computational
requirements for transcriptome assembly.}}

%add yeast

\begin{tabular}{|l|c|c|c|c|}

Data set & N reads pre/post & Assembly time pre/post & Assembly memory pre/post \\
 \hline \\
Yeast (Oases) & 100m / 9.3m & 181m / 12m (15.1x) & 45.2gb / 8.9gb (5.1x) \\
Yeast (Trinity) & 100m / 9.3m & 887 min / 145 min (6.1x) & 31.8gb / 10.4gb (3.1x) \\
Mouse (Oases) & 100m / 26.4m & 761 min/ 73 min (10.4x) & 116.0gb / 34.6gb (3.4x) \\
Mouse (Trinity) & 100m / 26.4m & 2297 min / 634 min (3.6x) & 42.1gb / 36.4gb (1.2x) \\
\end{tabular}

\begin{flushleft}
\end{flushleft}
\label{tab:dntrans}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[!ht]
\caption{
\bf{Digital normalization has assembler-specific effects on transcriptome
assembly.}}

%add yeast

\begin{tabular}{|l|c|c|c|c|}

Data set & Contigs $>$ 300 & Total bp $>$ 300 & Contigs $>$ 1000 & Total bp $>$ 1000 \\
\hline \\
Yeast (Oases) & 12,654 / 9,547 & 33.2mb / 27.7mb & 9,156 / 7,345 & 31.2mb / 26.4mb \\
Yeast (Trinity) & 10,344 / 12,092 & 16.2mb / 16.5mb & 5,765 / 6,053 & 13.6 mb / 13.1mb \\
Mouse (Oases) & 57,066 / 49,356 & 98.1mb / 84.9mb & 31,858 / 27,318 & 83.7mb / 72.4mb \\
Mouse (Trinity) & 50,801 / 61,242 & 79.6 mb / 78.8mb & 23,760 / 24,994 & 65.7mb / 59.4mb \\

\end{tabular}

\begin{flushleft}
\end{flushleft}
\label{tab:dntrans0}
\end{table}


\end{document}

% Candes, E.J., & Wakin, M.B., An Introduction To Compressive Sampling, IEEE Signal Processing Magazine, V.21, March 2008 [3]

% @error correction could be applied at C=20 stage to mrnaseq
